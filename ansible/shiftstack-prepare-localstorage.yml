---
# Playbook to prepare passthrough nvme storage for local storage operator
#
# Example Usage:
#
# time ansible-playbook -i ansible/inventory/hosts ansible/shiftstack-prepare-localstorage.yml
#
# Process on a node:
# (If previously used) Clean any outstanding symlinks in /mnt/local-storage/local-sc
# pvcreate /dev/nvme0n1
# vgcreate vg_ls /dev/nvme0n1
# lvcreate -l 99%VG -T vg_ls/lv_tp_ls
# lvcreate -T vg_ls/lv_tp_ls -V100G -n lv_tv00

- hosts: orchestration
  remote_user: stack
  vars_files:
    - vars/all.yml
  tasks:
    - name: Create temporary floating ip for master-0
      shell: |
        source /home/stack/ocp_clusters/{{ shiftstack_cluster }}/shiftstackrc
        openstack floating ip create public -f value -c floating_ip_address
      register: tmp_fip

    - name: Get master-0 name
      shell: |
        source /home/stack/ocp_clusters/{{ shiftstack_cluster }}/shiftstackrc
        openstack server list -f value -c Name | grep master-0
      register: master0_name

    - name: Associate floating ip to master-0
      shell: |
        source /home/stack/ocp_clusters/{{ shiftstack_cluster }}/shiftstackrc
        openstack server add floating ip {{ master0_name.stdout }} {{ tmp_fip.stdout }}

    - name: Get NVMe worker node ip addresses
      shell: |
        oc --kubeconfig=/home/stack/ocp_clusters/{{ shiftstack_cluster }}/auth/kubeconfig get no -l {{ shiftstack_local_storage_node_selector }} -o go-template={%raw%}'{{ range .items }}{{ range .status.addresses }}{{ if eq .type "InternalIP" }}{{ printf "%s\n" .address }}{{ end }}{{ end }}{{ end }}'{%endraw%}
      register: worker_node_ips

    - name: Get cluster ssh key
      fetch:
        src: /home/stack/.ssh/id_rsa
        dest: "{{ playbook_dir }}/.cluster_key"
        flat: true

    - name: Add worker to inventory via proxy
      add_host:
        name: "{{ item }}"
        group: local_storage_workers
        ansible_host: "{{ item }}"
        ansible_ssh_private_key_file: "{{ playbook_dir }}/.cluster_key"
        ansible_user: "core"
        ansible_ssh_common_args: |-
          -o ControlMaster=no -o ControlPersist=no -o ProxyCommand='ssh -i {{ playbook_dir }}/.cluster_key -W %h:%p core@{{ tmp_fip.stdout }}'
      with_items: "{{ worker_node_ips.stdout_lines }}"

- hosts: local_storage_workers
  gather_facts: false
  vars_files:
    - vars/all.yml
  tasks:
    - name: Ensure disks are wiped before running any lvm commands
      become: true
      shell: |
        wipefs -a -f {{ shiftstack_local_storage_device }}
      register: disk_wipe

    - name: Show disk wiping output
      debug:
        msg: "{{ disk_wipe.stdout }}"

    - name: Create LVM Volume Group
      become: true
      lvg:
        vg: vg_ls
        pvs: "{{ shiftstack_local_storage_device }}"

    - name: Create LVM Thin Pool
      become: true
      lvol:
        vg: vg_ls
        thinpool: lv_tp_ls
        size: 99%VG

    - name: Create LVM Thin Volumes
      become: true
      lvol:
        vg: vg_ls
        lv: "lv_tv{{ item }}"
        thinpool: lv_tp_ls
        size: "{{ shiftstack_thin_volume_size }}"
      with_sequence: "start=0 count={{ shiftstack_thin_volume_count|int }} format=%02u"

- hosts: orchestration
  remote_user: stack
  vars_files:
    - vars/all.yml
  tasks:
    - name: Delete temporary floating ip for master-0
      shell: |
        source /home/stack/ocp_clusters/{{ shiftstack_cluster }}/shiftstackrc
        openstack floating ip delete {{ tmp_fip.stdout }}
      register: tmp_fip
